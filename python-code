import pandas as pd
import numpy as np
import statsmodels.api as sm
import matplotlib.pyplot as plt
from scipy.optimize import minimize
from IPython.display import display # Necessary for use in Jupyter/Colab environments
import warnings

# Suppress minor statistical warnings for cleaner output
warnings.filterwarnings("ignore", module="statsmodels")

# =========================================================================
# 1. CONFIGURATION & GLOBAL PARAMETERS
# =========================================================================

# --- Intervention Date ---
# *** REQUIRED: Set the date when the intervention (New Home Page) was launched. ***
NEW_HOME_PAGE_DATE = pd.to_datetime('2025-09-11')

# --- Plotting Colors ---
# Define custom colors for consistent visualization across the analysis
COLOR_MAP = {
    'Home Page': '#593CFB',
    'US Country Hub LP': '#3082E8',
    'Cities, Airports, Makes LPs': '#C8ACFF',
    'Other': '#FFB144',
    'Synthetic Control': '#6CF6BC'
}
LAUNCH_LINE_COLOR = '#6CF6BC' # Color for the vertical intervention line

# --- Analysis Metrics ---
# List of all final outcome metrics used for DiD modeling
DID_METRICS = [
    'roas_platform', 'session_2_book', 'search_2_book', 'session_2_search',
    'search_2_checkoutpage', 'roas_internal'
]

# --- Pandas Display Options ---
pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)


# =========================================================================
# 2. DATA CLEANING AND PRE-PROCESSING FUNCTIONS
# =========================================================================

def get_human_readable_label(metric: str) -> str:
    """Helper to generate human-readable labels for plots/tables."""
    label = metric.replace("_", " ").title()
    if metric == 'roas_platform':
        return 'ROAS (Return on Ad Spend)'
    if metric == 'roas_internal':
        return 'ROAS Internal (GBV Local / Spend)'
    return label

def clean_landing_page_data(df: pd.DataFrame, maturity_days: int = 4) -> pd.DataFrame:
    """
    Cleans, transforms, filters, and aggregates the raw landing page data.

    Args:
        df: The raw pandas DataFrame with landing page data (must contain 'Day', 'Landing page',
            and campaign/cost/conversion columns).
        maturity_days: Number of recent days to exclude due to data immaturity.
    """
    print("\n--- Running Landing Page Data Cleaning Pipeline ---")
    df_cleaned = df.copy()

    # Apply maturity filter: Remove the last X days of data
    if not df_cleaned.empty and 'Day' in df_cleaned.columns:
        df_cleaned['Day'] = pd.to_datetime(df_cleaned['Day'], errors='coerce')
        max_date = df_cleaned['Day'].max()
        cutoff_date = max_date - pd.Timedelta(days=maturity_days)
        df_cleaned = df_cleaned[df_cleaned['Day'] <= cutoff_date]
        print(f"Data filtered. Max date: {max_date.strftime('%Y-%m-%d')}. Cutoff date (excluding last {maturity_days} days): {cutoff_date.strftime('%Y-%m-%d')}.")

    # Clean and convert numeric types (handling commas and percentages)
    df_cleaned['CTR'] = df_cleaned['CTR'].astype(str).str.replace('%', '', regex=False).astype(float) / 100

    for col in ['Cost', 'Conv. value']:
        df_cleaned[col] = df_cleaned[col].astype(str).str.replace(',', '', regex=False).astype(float)

    for col in ['Impr.', 'Clicks', 'Conversions (Purchase Clean)']:
        df_cleaned[col] = df_cleaned[col].astype(str).str.replace(',', '', regex=False).astype(float).round(0).astype(pd.Int64Dtype())

    # Create 'lp_category' feature (Logic based on Turo LP structure)
    print("Creating 'lp_category' feature...")
    conditions = [
        df_cleaned['Landing page'].isin(['https://turo.com/us/en', 'www.turo.com']),
        df_cleaned['Landing page'].isin(['https://turo.com/us/en/car-rental/united-states', 'www.turo.com/us/en/car-rental/united-states']),
        df_cleaned['Landing page'].str.startswith('https://turo.com/us/en/car-rental/united-states/', na=False),
        df_cleaned['Landing page'].str.contains('exotic-luxury-rental|van-rental|suv-rental|convertible-rental|cargo-van-rental|truck-rental', regex=True, na=False),
        df_cleaned['Landing page'].str.contains('.com/ca/|.com/au/|.com/gb/|.com/fr/', regex=True, na=False),
        df_cleaned['Landing page'].str.contains('/gift-cards-landing', regex=False, na=False),
        df_cleaned['Landing page'].str.contains('/list-your-car', regex=False, na=False),
        df_cleaned['Landing page'].str.contains('/host-tools/deals-and-discounts', regex=False, na=False),
        df_cleaned['Landing page'].str.contains('https://turo.com/blog/', regex=False, na=False),
        df_cleaned['Landing page'].str.split('/', n=2, expand=True)[2].str.startswith('help.turo.com', na=False),
        df_cleaned['Landing page'].str.contains('/states/', regex=False, na=False),
        df_cleaned['Landing page'].str.contains('/search?location=', regex=False, na=False),
        df_cleaned['Landing page'].str.endswith('/search', na=False),
        df_cleaned['Landing page'].str.contains('how-turo-works', regex=False, na=False),
        df_cleaned['Landing page'].str.contains('get-started', regex=False, na=False)
    ]
    values = [
        'Home Page', 'US Country Hub LP', 'Cities, Airports, Makes LPs', 'Rent Categories LPs',
        'International sites', 'Gift Card Landing Page', 'List Your Car', 'Deals and Discounts',
        'Blog Article', 'Help Center', 'State Page', 'Search w/ Filters', 'Search (SRP)',
        'How Turo Works (HTW)', 'Core SEM LP'
    ]
    df_cleaned['lp_category'] = np.select(conditions, values, default='Other')

    # Group and aggregate data
    df_grouped = df_cleaned.groupby(['Day', 'lp_category']).agg(
        Cost=('Cost', 'sum'), Impr=('Impr.', 'sum'), Clicks=('Clicks', 'sum'),
        Conversions_Purchase_Clean=('Conversions (Purchase Clean)', 'sum'), Conv_value=('Conv. value', 'sum')
    ).reset_index()

    # Rename and compute initial features
    df_grouped = df_grouped.rename(columns={
        'Cost': 'spend_usd', 'Impr': 'impressions', 'Clicks': 'clicks',
        'Conversions_Purchase_Clean': 'bookings_platform', 'Conv_value': 'gbv_platform'
    })
    df_grouped['roas_platform'] = np.where(df_grouped['spend_usd'] != 0, df_grouped['gbv_platform'] / df_grouped['spend_usd'], 0)
    df_grouped['ctr'] = np.where(df_grouped['impressions'] != 0, df_grouped['clicks'] / df_grouped['impressions'], 0)

    # Final column selection
    df_final = df_grouped[['Day', 'lp_category', 'spend_usd', 'impressions', 'clicks', 'ctr', 'bookings_platform', 'gbv_platform', 'roas_platform']]

    print("Landing page data cleaning complete.")
    return df_final

def merge_dataframes(df_raw_sessions: pd.DataFrame, df_cleaned_lp: pd.DataFrame) -> pd.DataFrame:
    """
    Cleans the raw sessions DataFrame and merges it with the cleaned landing page DataFrame.
    Calculates key derived ratio metrics (roas_internal, session_2_search).
    """
    print("\n--- Running Sessions and Merge Pipeline ---")
    df_cleaned_sessions = df_raw_sessions.copy()

    # 1. Rename columns and clean date (assuming session_date/landing_experience_type are standard)
    df_cleaned_sessions = df_cleaned_sessions.rename(columns={
        'session_date': 'Day',
        'landing_experience_type': 'lp_category'
    })
    df_cleaned_sessions['Day'] = pd.to_datetime(df_cleaned_sessions['Day'], errors='coerce')

    # 2. Merge the two dataframes
    print("Merging dataframes on 'Day' and 'lp_category'...")
    merged_df = pd.merge(df_cleaned_lp, df_cleaned_sessions, on=['Day', 'lp_category'], how='left')

    # 3. Compute new ratio features that require merged data
    print("Computing roas_internal and session_2_search...")

    # Identify session column name (either 'sessions' or 'total_sessions')
    session_col = 'sessions' if 'sessions' in merged_df.columns else 'total_sessions'

    # Calculate session_2_search
    merged_df['session_2_search'] = np.where(
        merged_df[session_col].fillna(0) != 0,
        merged_df['search'] / merged_df[session_col], 0
    )

    # Calculate roas_internal
    merged_df['roas_internal'] = np.where(
        merged_df['spend_usd'].fillna(0) != 0,
        merged_df['gbv_local'] / merged_df['spend_usd'], 0
    )

    # 4. Fill NaN values in new ratio columns with 0 after merge
    ratio_cols = [
        'session_2_book', 'search_2_book', 'search_2_checkoutpage',
        'session_2_search', 'roas_internal', 'gbv_local', 'net_revenue_local',
        session_col, 'search', 'car_page', 'intent', 'checkout_page', 'checkout_clickcontact'
    ]
    for col in ratio_cols:
        if col in merged_df.columns:
            merged_df[col] = merged_df[col].fillna(0)

    print("Merging pipeline complete.")
    return merged_df

# =========================================================================
# 3. STATISTICAL AND MODELING FUNCTIONS
# =========================================================================

def create_synthetic_control(df: pd.DataFrame, treated_group: str, control_options: list, outcome_metric: str) -> pd.DataFrame:
    """
    Creates a 'Synthetic Control' group by optimally weighting control_options
    to match the treated_group's pre-intervention trend for the outcome_metric.
    """
    print(f"\n--- Creating Synthetic Control for {outcome_metric} (Treated: {treated_group}) ---")

    pre_period_df = df[(df['Day'] < NEW_HOME_PAGE_DATE)].copy()
    treated_df = pre_period_df[pre_period_df['lp_category'] == treated_group].sort_values('Day')
    control_df = pre_period_df[pre_period_df['lp_category'].isin(control_options)].sort_values('Day')

    if treated_df.empty: return df

    control_pivot = control_df.pivot(index='Day', columns='lp_category', values=outcome_metric)
    treated_outcome = treated_df[outcome_metric].reindex(control_pivot.index).fillna(treated_df[outcome_metric].mean())
    control_pivot = control_pivot.fillna(0)

    # Optimization Objective: Minimize the distance between treated and weighted controls
    def objective(weights, treated_y, control_x):
        weights = np.clip(weights, 0, 1)
        synthetic_y = np.dot(control_x, weights)
        return np.sum((treated_y - synthetic_y) ** 2)

    n_controls = control_pivot.shape[1]
    bounds = [(0, 1) for _ in range(n_controls)]
    constraints = ({'type': 'eq', 'fun': lambda w: np.sum(w) - 1})
    initial_weights = np.ones(n_controls) / n_controls

    optimization_result = minimize(
        objective, initial_weights, args=(treated_outcome, control_pivot.values),
        method='SLSQP', bounds=bounds, constraints=constraints
    )

    if not optimization_result.success:
        print("Warning: Synthetic control optimization failed.")
        return df

    optimized_weights = pd.Series(optimization_result.x, index=control_pivot.columns)
    optimized_weights = optimized_weights[optimized_weights > 1e-6]

    print("Optimized Control Weights:")
    print(optimized_weights.sort_values(ascending=False))

    # Apply weights to the full dataset
    full_pivot = df.pivot(index='Day', columns='lp_category', values=outcome_metric)
    synthetic_series = np.dot(full_pivot[optimized_weights.index].fillna(0), optimized_weights)

    # Create the Synthetic Control DataFrame and merge back
    synthetic_df = pd.DataFrame({'Day': synthetic_series.index, outcome_metric: synthetic_series.values})
    all_cols = list(df.columns)
    synthetic_df = synthetic_df.reindex(columns=all_cols)
    synthetic_df['lp_category'] = 'Synthetic Control'

    df_merged_sc = pd.concat([df, synthetic_df], ignore_index=True)

    new_sc_df = df_merged_sc[(df_merged_sc['lp_category'].isin([treated_group, 'Synthetic Control'])) & (df_merged_sc['Day'] < NEW_HOME_PAGE_DATE)].pivot(index='Day', columns='lp_category', values=outcome_metric)
    new_corr = new_sc_df[treated_group].corr(new_sc_df['Synthetic Control'])
    print(f"New Correlation for Synthetic Control: {new_corr:.4f}")

    return df_merged_sc

def analyze_did_assumptions(df: pd.DataFrame, did_metrics: list) -> pd.DataFrame:
    """
    Analyzes the pre-period trend similarity (correlation) between the treated
    and potential control groups for each outcome metric.
    """
    print("\n" + "="*70)
    print("--- DiD Assumption Analysis: Control Group Selection ---")
    print("="*70)

    treated_group = 'Home Page'
    all_categories = df['lp_category'].unique()

    # Define primary control options (excluding low-volume, non-comparable pages)
    control_options = [c for c in all_categories if c not in [
        treated_group, 'Gift Card Landing Page', 'State Page', 'Blog Article', 'Help Center', 'Core SEM LP', 'International sites'
    ]]

    # Ensure Synthetic Control is included if generated
    if 'Synthetic Control' in all_categories:
        control_options = [c for c in control_options if c != 'Synthetic Control']
        control_options.append('Synthetic Control')

    pre_period_df = df[(df['Day'] < NEW_HOME_PAGE_DATE)].copy()
    results = []

    for metric in did_metrics:
        for control in control_options:
            filtered_df = pre_period_df[pre_period_df['lp_category'].isin([treated_group, control])]
            pivot_df = filtered_df.pivot(index='Day', columns='lp_category', values=metric)

            if treated_group in pivot_df.columns and control in pivot_df.columns:
                corr = pivot_df[treated_group].corr(pivot_df[control])
            else:
                corr = np.nan

            results.append({
                'Treated Group': treated_group,
                'Control Group': control,
                'Outcome Metric': metric,
                'Pre-Period Correlation (Similarity)': corr
            })

    results_df = pd.DataFrame(results).sort_values(
        by='Pre-Period Correlation (Similarity)', ascending=False
    ).dropna(subset=['Pre-Period Correlation (Similarity)'])

    print("\n[2] Pre-Period Trend Similarity by Metric (Best Control Group has Correlation closest to 1.0000):")
    display(results_df.head(20).style.format({'Pre-Period Correlation (Similarity)': '{:.4f}'}))

    return results_df

def run_did_regression(df: pd.DataFrame, treated_group: str, control_group: str, outcome_metric: str, time_name='Day'):
    """
    Runs an Extended Difference-in-Differences (DiD) OLS regression with
    Group-Specific Linear Time Trends and Cluster-Robust Standard Errors.
    """
    print("\n" + "="*70)
    print(f"--- Running DiD Regression: Outcome={outcome_metric} (Treated={treated_group}, Control={control_group}) ---")
    print("="*70)

    analysis_df = df[df['lp_category'].isin([treated_group, control_group])].copy()

    # Create DiD Variables
    analysis_df['Treated'] = (analysis_df['lp_category'] == treated_group).astype(int)
    analysis_df['Post'] = (analysis_df[time_name] >= NEW_HOME_PAGE_DATE).astype(int)
    analysis_df['DID'] = analysis_df['Treated'] * analysis_df['Post']
    analysis_df['Time'] = (analysis_df[time_name] - analysis_df[time_name].min()).dt.days
    analysis_df['Time:Treated'] = analysis_df['Time'] * analysis_df['Treated']

    # Define Formula (5-term Extended DiD)
    formula = f"{outcome_metric} ~ Treated + Post + DID + Time + Time:Treated"
    print(f"Regression Formula: {formula}")

    try:
        model = sm.formula.ols(formula, data=analysis_df)

        # Fit the model using Cluster-Robust Standard Errors (clustered by lp_category)
        results = model.fit(
            cov_type='cluster',
            cov_kwds={'groups': analysis_df['lp_category']}
        )

        print("\nModel Summary (using Cluster-Robust Standard Errors):")
        display(results.summary())

        did_coefficient = results.params['DID']
        print("\n--- Key Causal Result (Difference-in-Differences Coefficient - ATT) ---")
        print(f"Estimated Treatment Effect (DID Coefficient): {did_coefficient:.4f}")

        return results

    except Exception as e:
        print(f"\nAn error occurred during regression: {e}")
        return None

# =========================================================================
# 4. PLOTTING FUNCTIONS
# =========================================================================

def plot_metric_timeline(df: pd.DataFrame, interest_metric: str, test_group_set: list):
    """Creates a matplotlib timeline visualization for a specific pair of categories."""
    print(f"\nGenerating timeline visualization for {interest_metric}...")

    plot_df = df[df['lp_category'].isin(test_group_set)].copy()

    plt.style.use('ggplot')
    fig, ax = plt.subplots(figsize=(15, 8))

    y_label = get_human_readable_label(interest_metric)

    for category, group in plot_df.groupby('lp_category'):
        color = COLOR_MAP.get(category, '#000000')
        ax.plot(group['Day'], group[interest_metric], label=category, color=color)

    ax.axvline(x=NEW_HOME_PAGE_DATE, color=LAUNCH_LINE_COLOR, linestyle='--', linewidth=2, label='New Home Page Launch')

    title = f'{y_label} Over Time for {test_group_set[0]} vs. {test_group_set[1]}'
    ax.set_title(title)
    ax.set_xlabel('Date')
    ax.set_ylabel(y_label)
    ax.legend()
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    display(fig)
    plt.close(fig)

def plot_metric_subplots(df: pd.DataFrame, interest_metric: str, test_group_set: list):
    """Creates a series of matplotlib subplots, one for each group in the test_group_set."""
    print(f"\nGenerating paired subplots for {interest_metric}...")
    plot_df = df[df['lp_category'].isin(test_group_set)].copy()

    plt.style.use('ggplot')
    fig, axes = plt.subplots(
        nrows=len(test_group_set), ncols=1,
        figsize=(15, 5 * len(test_group_set)),
        sharex=True
    )

    y_label = get_human_readable_label(interest_metric)

    for i, category in enumerate(test_group_set):
        group = plot_df[plot_df['lp_category'] == category]
        ax = axes[i]
        color = COLOR_MAP.get(category, '#000000')

        ax.plot(group['Day'], group[interest_metric], label=category, color=color)

        ax.axvline(x=NEW_HOME_PAGE_DATE, color=LAUNCH_LINE_COLOR, linestyle='--', linewidth=2, label='New Home Page Launch')

        ax.set_title(f'{category} | {y_label} Over Time')
        ax.set_ylabel(y_label)
        ax.legend()

        if i == len(test_group_set) - 1: # Only label the x-axis on the bottom plot
            ax.set_xlabel('Date')
            ax.tick_params(axis='x', rotation=45)
        else:
            ax.tick_params(axis='x', labelbottom=False)

    plt.tight_layout()
    display(fig)
    plt.close(fig)

def plot_did_summary(results, outcome_metric: str):
    """
    Creates the classic four-point DiD summary plot using OLS coefficients.
    Assumes a standard 5-term DiD model (Treated, Post, DID, Time, Time:Treated).
    """
    if results is None or not hasattr(results, 'params'):
        print("Cannot plot: Regression results object is invalid or empty.")
        return

    print(f"\n--- Generating 4-Point DiD Summary Plot for {outcome_metric} ---")

    # Extract coefficients
    b0 = results.params['Intercept']
    b1 = results.params['Treated']
    b2 = results.params['Post']
    b3 = results.params['DID']

    periods = [0, 1]

    # Calculate the four corner points
    c_pre = b0                              # Control Pre (B0)
    c_post = b0 + b2                        # Control Post (B0 + B2)
    t_pre = b0 + b1                         # Treated Pre (B0 + B1)
    t_post = b0 + b1 + b2 + b3              # Treated Post (B0 + B1 + B2 + B3)

    # Counterfactual (T_counter)
    t_counter = t_pre + (c_post - c_pre)    # T_pre + (Change in Control)

    plt.style.use('ggplot')
    fig, ax = plt.subplots(figsize=(8, 6))

    # Plot Control Group (Black)
    ax.plot(periods, [c_pre, c_post], marker='o', linestyle='-', color='black', label='Control (Actual)')

    # Plot Treated Group (Home Page Color)
    ax.plot(periods, [t_pre, t_post], marker='o', linestyle='-', color=COLOR_MAP.get('Home Page'), label='Treated (Actual)')

    # Plot Counterfactual (Dashed)
    ax.plot(periods, [t_pre, t_counter], marker='o', linestyle='--', color='purple', label='Counterfactual')

    # Add DID effect line
    did_color = 'red' if b3 < 0 else 'green'
    ax.vlines(1, t_counter, t_post, colors=did_color, linestyles=':', linewidth=2, label=f'DID Effect: {b3:.4f}')

    # Formatting
    ax.set_xticks(periods)
    ax.set_xticklabels(['Pre-Intervention', 'Post-Intervention'])
    ax.set_title(f'DiD Causal Effect Summary: {get_human_readable_label(outcome_metric)}')
    ax.set_ylabel(get_human_readable_label(outcome_metric))
    ax.legend(title="Group", loc='best')
    ax.grid(True)

    display(fig)
    plt.close(fig)


if __name__ == '__main__':
    # --- Example Usage (Including Mock Data for Testing) ---

    # This block requires you to replace the mock data with your actual dataframes

    # 1. Create Mock Data (Replace with your actual pandas DataFrames)

    # Generate mock data that spans 60 days pre-intervention (Aug 1 - Sep 10) and 20 days post (Sep 11 - Sep 30)
    date_range = pd.date_range(start='2025-08-01', end='2025-09-30', name='Day')

    # Create structured mock data for 4 key categories
    categories = ['Home Page', 'Cities, Airports, Makes LPs', 'Other', 'Search (SRP)']

    data_list = []
    for cat in categories:
        df_cat = pd.DataFrame(date_range)
        df_cat['lp_category'] = cat

        # Simulate different trends for the DID test
        cost_base = 25 if cat == 'Home Page' else 15
        cost_multiplier = 1.05 if cat == 'Home Page' else 1.0

        df_cat['Cost'] = cost_base + np.arange(len(date_range)) * cost_multiplier * np.random.uniform(0.1, 0.5)
        df_cat['Impr.'] = np.random.randint(500, 2000, len(date_range))
        df_cat['Clicks'] = np.random.randint(50, 150, len(date_range))
        df_cat['Conv. value'] = 100 * df_cat['Clicks'] * np.random.uniform(1.2, 1.5)
        df_cat['Conversions (Purchase Clean)'] = df_cat['Clicks'] / 10

        # Simulate ROAS drop for Home Page post-intervention
        post_intervention = df_cat['Day'] >= NEW_HOME_PAGE_DATE
        if cat == 'Home Page':
            df_cat.loc[post_intervention, 'Cost'] *= 2.0  # Double cost post-launch
            df_cat.loc[post_intervention, 'Conversions (Purchase Clean)'] *= 1.1 # Small conversion increase

        df_cat['CTR'] = [f"{x:.1f}%" for x in np.random.uniform(5, 10, len(date_range))]

        data_list.append(df_cat)

    df_raw_lp_mock = pd.concat(data_list)
    df_raw_lp_mock = df_raw_lp_mock.rename(columns={'Day': 'Day', 'Cost': 'Cost', 'Impr.': 'Impr.',
                                                    'Clicks': 'Clicks', 'Conversions (Purchase Clean)': 'Conversions (Purchase Clean)',
                                                    'Conv. value': 'Conv. value', 'lp_category': 'Landing page'})

    # Create Mock Sessions Data
    sessions_data_list = []
    for cat in categories:
        df_cat_s = pd.DataFrame(date_range)
        df_cat_s['landing_experience_type'] = cat
        df_cat_s['sessions'] = np.random.randint(5000, 20000, len(date_range))
        df_cat_s['search'] = df_cat_s['sessions'] * np.random.uniform(0.1, 0.5) # Simulate S2S
        df_cat_s['gbv_local'] = 100 * df_cat_s['search'] * np.random.uniform(2.0, 3.0) # Simulate GBV Local
        df_cat_s['search_2_checkoutpage'] = np.random.uniform(0.1, 0.2, len(date_range))
        df_cat_s['search_2_book'] = np.random.uniform(0.01, 0.05, len(date_range))
        df_cat_s['session_2_book'] = df_cat_s['search_2_book'] * df_cat_s['search'] / df_cat_s['sessions']
        df_cat_s['net_revenue_local'] = df_cat_s['gbv_local'] * 0.1
        df_cat_s['session_date'] = df_cat_s['Day']

        # Simulate top-funnel drop for Home Page post-intervention
        post_intervention = df_cat_s['Day'] >= NEW_HOME_PAGE_DATE
        if cat == 'Home Page':
             df_cat_s.loc[post_intervention, 'search'] *= 0.7 # Simulate 30% drop in S2S efficiency

        sessions_data_list.append(df_cat_s)

    df_raw_sessions_mock = pd.concat(sessions_data_list)
    df_raw_sessions_mock = df_raw_sessions_mock.drop(columns=['Day'])

    # --- Actual Pipeline Execution ---

    # Step 1: Clean and aggregate LP data
    df_cleaned_lp = clean_landing_page_data(df_raw_lp_mock)

    # Step 2: Merge LP and Sessions data, compute final ratios
    final_df = merge_dataframes(df_raw_sessions_mock, df_cleaned_lp)

    # Step 3: Run Synthetic Control for a key metric (e.g., S2S)
    control_groups_for_sc = ['US Country Hub LP', 'Cities, Airports, Makes LPs', 'Other', 'Search (SRP)']
    final_df = create_synthetic_control(final_df, 'Home Page', control_groups_for_sc, 'session_2_search')

    # Step 4: Define Final DiD Models and Run Regressions
    final_did_models_run = {
        'session_2_search': {'control': 'Cities, Airports, Makes LPs', 'groups': ['Home Page', 'Cities, Airports, Makes LPs']},
        'search_2_checkoutpage': {'control': 'Other', 'groups': ['Home Page', 'Other']},
        'search_2_book': {'control': 'Other', 'groups': ['Home Page', 'Other']},
        'roas_platform': {'control': 'Other', 'groups': ['Home Page', 'Other']},
        'roas_internal': {'control': 'Search (SRP)', 'groups': ['Home Page', 'Search (SRP)']}
    }

    print("\n" + "="*70)
    print("--- Running Final DiD Regression Models ---")
    print("="*70)

    for metric, spec in final_did_models_run.items():
        results = run_did_regression(final_df, 'Home Page', spec['control'], metric)
        spec['results'] = results # Store results object

    # Step 5: Visualization (using only the 4 most critical models)
    plotting_pairs = {
        'session_2_search': ['Home Page', final_did_models_run['session_2_search']['control']],
        'search_2_checkoutpage': ['Home Page', final_did_models_run['search_2_checkoutpage']['control']],
        'search_2_book': ['Home Page', final_did_models_run['search_2_book']['control']],
        'roas_platform': ['Home Page', final_did_models_run['roas_platform']['control']]
    }

    # 5.1 Timeline Visualizations
    for metric, groups in plotting_pairs.items():
        plot_metric_timeline(final_df, metric, groups)

    # 5.2 Subplot Visualizations
    for metric, groups in plotting_pairs.items():
        plot_metric_subplots(final_df, metric, groups)

    # 5.3 DiD Summary Plot
    for metric, spec in final_did_models_run.items():
        if metric in plotting_pairs and spec['results'] is not None:
            plot_did_summary(spec['results'], metric)
